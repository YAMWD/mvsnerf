{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f6e329",
   "metadata": {},
   "source": [
    "# MVSNeRF DTU Reconstruction\n",
    "This notebook takes several photos from different views (DTU dataset) and reconstructs a 3D point cloud and mesh using MVSNeRF. It saves interactive HTML visualizations (point cloud and mesh) that you can open in a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21781406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /ibex/user/yaoz0b/conda-environments/mvsnerf/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in /ibex/user/yaoz0b/conda-environments/mvsnerf/lib/python3.8/site-packages (from scikit-image) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.8 in /ibex/user/yaoz0b/conda-environments/mvsnerf/lib/python3.8/site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /ibex/user/yaoz0b/conda-environments/mvsnerf/lib/python3.8/site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /ibex/user/yaoz0b/conda-environments/mvsnerf/lib/python3.8/site-packages (from scikit-image) (9.5.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /ibex/user/yaoz0b/conda-environments/mvsnerf/lib/python3.8/site-packages (from scikit-image) (2.35.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /ibex/user/yaoz0b/conda-environments/mvsnerf/lib/python3.8/site-packages (from scikit-image) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /ibex/user/yaoz0b/conda-environments/mvsnerf/lib/python3.8/site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in /ibex/user/yaoz0b/conda-environments/mvsnerf/lib/python3.8/site-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /ibex/user/yaoz0b/conda-environments/mvsnerf/lib/python3.8/site-packages (from scikit-image) (0.4)\n",
      "Dependencies ready.\n"
     ]
    }
   ],
   "source": [
    "# Optional: install visualization dependencies if missing\n",
    "import sys, subprocess\n",
    "def ensure(pkgs):\n",
    "    for p in pkgs:\n",
    "        try:\n",
    "            __import__(p.split('==')[0].replace('-', '_'))\n",
    "        except Exception:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', p])\n",
    "ensure(['plotly', 'scikit-image', 'trimesh'])\n",
    "print('Dependencies ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f70aa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "DTU_ROOT: /ibex/user/yaoz0b/mvs_training/dtu\n",
      "SCAN: scan1\n",
      "Checkpoint: runs_fine_tuning/scan1-ft/ckpts/latest.tar\n"
     ]
    }
   ],
   "source": [
    "# Imports and configuration\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import torch\n",
    "from types import SimpleNamespace\n",
    "from skimage.measure import marching_cubes\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "# Repo modules\n",
    "from data.dtu import MVSDatasetDTU\n",
    "from models import create_nerf_mvs, RefVolume\n",
    "from renderer import render_density\n",
    "from utils import get_ptsvolume, build_color_volume\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# Paths and parameters (edit as needed)\n",
    "DTU_ROOT = os.environ.get('DTU_ROOT', '/ibex/user/yaoz0b/mvs_training/dtu')  # e.g., '/data/DTU'\n",
    "SCAN = os.environ.get('DTU_SCAN', 'scan1')\n",
    "N_VIEWS = int(os.environ.get('DTU_N_VIEWS', '4'))  # total views including reference\n",
    "DOWNSAMPLE = float(os.environ.get('DTU_DOWNSAMPLE', '1.0'))\n",
    "PAD = int(os.environ.get('DTU_PAD', '24'))\n",
    "CKPT_PATH = os.environ.get('MVSNERF_CKPT', 'runs_fine_tuning/scan1-ft/ckpts/latest.tar')\n",
    "OUT_DIR = Path('results/mvsnerf_dtu')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('DTU_ROOT:', DTU_ROOT)\n",
    "print('SCAN:', SCAN)\n",
    "print('Checkpoint:', CKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba1d14f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> image down scale: 1.0\n",
      "Images: (1, 4, 3, 512, 640) Proj: (1, 4, 3, 4) Near/Far: [2.125, 4.525000095367432]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2534181/1877182704.py:45: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Patch MVSDatasetDTU to fix proj_mats stacking issue\n",
    "def patched_build_proj_mats(self):\n",
    "    proj_mats, intrinsics, world2cams, cam2worlds = [], [], [], []\n",
    "    for vid in self.id_list:\n",
    "        proj_mat_filename = os.path.join(self.root_dir, f'Cameras/train/{vid:08d}_cam.txt')\n",
    "        intrinsic, extrinsic, near_far = self.read_cam_file(proj_mat_filename)\n",
    "        intrinsic[:2] *= 4\n",
    "        extrinsic[:3, 3] *= self.scale_factor\n",
    "        intrinsic[:2] = intrinsic[:2] * self.downSample\n",
    "        intrinsics += [intrinsic.copy()]\n",
    "        proj_mat_l = np.eye(4)\n",
    "        intrinsic[:2] = intrinsic[:2] / 4\n",
    "        proj_mat_l[:3, :4] = intrinsic @ extrinsic[:3, :4]\n",
    "        proj_mats += [(proj_mat_l, near_far)]\n",
    "        world2cams += [extrinsic]\n",
    "        cam2worlds += [np.linalg.inv(extrinsic)]\n",
    "    # Fix: separate tuples before stacking\n",
    "    proj_mats_only = np.stack([pm[0] for pm in proj_mats])\n",
    "    near_fars = np.stack([pm[1] for pm in proj_mats])\n",
    "    self.proj_mats = list(zip(proj_mats_only, near_fars))\n",
    "    self.intrinsics = np.stack(intrinsics)\n",
    "    self.world2cams, self.cam2worlds = np.stack(world2cams), np.stack(cam2worlds)\n",
    "\n",
    "# Apply patch before instantiation\n",
    "MVSDatasetDTU.build_proj_mats = patched_build_proj_mats\n",
    "\n",
    "# Load DTU dataset sample for the selected scan\n",
    "dataset = MVSDatasetDTU(root_dir=DTU_ROOT, split='val', n_views=N_VIEWS, downSample=DOWNSAMPLE)\n",
    "\n",
    "# Find an index corresponding to the desired scan\n",
    "scan_indices = [i for i, m in enumerate(dataset.metas) if m[0] == SCAN]\n",
    "if len(scan_indices) == 0:\n",
    "    raise RuntimeError(f'No entries found for scan {SCAN}. Available scans: {dataset.scans[:5]} ...')\n",
    "idx = scan_indices[1]\n",
    "sample = dataset[idx]\n",
    "\n",
    "# Pack pose information expected by renderer/utils\n",
    "pose_source = {\n",
    "    'w2cs': torch.tensor(sample['w2cs'], dtype=torch.float32, device=DEVICE),\n",
    "    'c2ws': torch.tensor(sample['c2ws'], dtype=torch.float32, device=DEVICE),\n",
    "    'intrinsics': torch.tensor(sample['intrinsics'], dtype=torch.float32, device=DEVICE),\n",
    "    'near_fars': torch.tensor(sample['near_fars'], dtype=torch.float32, device=DEVICE)\n",
    "}\n",
    "\n",
    "imgs = torch.tensor(sample['images'], dtype=torch.float32, device=DEVICE).unsqueeze(0)  # [1, V, 3, H, W]\n",
    "proj_mats = torch.tensor(sample['proj_mats'], dtype=torch.float32, device=DEVICE).unsqueeze(0)  # [1, V, 3, 4]\n",
    "near_far_source = torch.tensor(sample['near_fars'][0], dtype=torch.float32, device=DEVICE)  # [2]\n",
    "print('Images:', tuple(imgs.shape), 'Proj:', tuple(proj_mats.shape), 'Near/Far:', near_far_source.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62add25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ckpts ['runs_fine_tuning/scan1-ft/ckpts/latest.tar']\n",
      "Reloading from runs_fine_tuning/scan1-ft/ckpts/latest.tar\n",
      "Models ready.\n"
     ]
    }
   ],
   "source": [
    "# Initialize MVSNeRF with checkpoint\n",
    "from opt import config_parser\n",
    "\n",
    "# Build args via config_parser to satisfy model creation\n",
    "args = config_parser(cmd=[\n",
    "    '--expname', 'dtu_recon',\n",
    "    '--dataset_name', 'dtu',\n",
    "    '--net_type', 'v2',\n",
    "    '--multires', '10',\n",
    "    '--multires_views', '4',\n",
    "    '--N_samples', '128',\n",
    "    '--N_importance', '0',\n",
    "    '--netchunk', '2048',\n",
    "    '--chunk', '1024',\n",
    "    '--pad', str(PAD),\n",
    "    '--raw_noise_std', '0.0',\n",
    "    '--ckpt', CKPT_PATH\n",
    "])\n",
    "# Additional fields expected by models/renderers\n",
    "args.pts_dim = 3\n",
    "args.dir_dim = 3\n",
    "# Use 3 views for color features to match checkpoint (8 + 3*4 = 20)\n",
    "FEATURE_VIEWS = 3\n",
    "args.feat_dim = 8 + FEATURE_VIEWS * 4\n",
    "args.use_viewdirs = False\n",
    "args.white_bkgd = False\n",
    "args.use_color_volume = True\n",
    "args.use_density_volume = True\n",
    "\n",
    "render_kwargs_train, render_kwargs_test, _, _ = create_nerf_mvs(args, use_mvs=True, dir_embedder=False, pts_embedder=True)\n",
    "MVSNet = render_kwargs_train['network_mvs']\n",
    "network_fn = render_kwargs_train['network_fn']\n",
    "network_query_fn = render_kwargs_train['network_query_fn']\n",
    "print('Models ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "692dbae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume feature: (1, 8, 128, 176, 208)\n",
      "Density volume: (128, 176, 208)\n"
     ]
    }
   ],
   "source": [
    "# Build the feature volume and density volume\n",
    "with torch.no_grad():\n",
    "    volume_feature, img_feats, depth_values = MVSNet(imgs, proj_mats, near_far_source, pad=PAD, lindisp=args.use_disp)\n",
    "    print('Volume feature:', tuple(volume_feature.shape))\n",
    "\n",
    "# Create reference volume wrapper\n",
    "ref_volume = RefVolume(volume_feature.detach()).to(DEVICE)\n",
    "\n",
    "# Prepare voxel grid in world coordinates\n",
    "D, H, W = volume_feature.shape[-3:]\n",
    "intrinsic_ref = pose_source['intrinsics'][0].clone()\n",
    "c2w_ref = pose_source['c2ws'][0]\n",
    "intrinsic_ref[:2] /= 4\n",
    "vox_pts = get_ptsvolume(H - 2 * PAD, W - 2 * PAD, D, PAD, near_far_source, intrinsic_ref, c2w_ref)\n",
    "# Unnormalize images for color projection\n",
    "def unpreprocess(data, shape=(1,1,3,1,1)):\n",
    "    device = data.device\n",
    "    mean = torch.tensor([-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225]).view(*shape).to(device)\n",
    "    std = torch.tensor([1 / 0.229, 1 / 0.224, 1 / 0.225]).view(*shape).to(device)\n",
    "    return (data - mean) / std\n",
    "imgs_un = unpreprocess(imgs)\n",
    "\n",
    "# Limit color features to FEATURE_VIEWS to match checkpoint feature dim\n",
    "imgs_un_feat = imgs_un[:, :FEATURE_VIEWS]\n",
    "pose_source_feat = {\n",
    "    'w2cs': pose_source['w2cs'][:FEATURE_VIEWS],\n",
    "    'c2ws': pose_source['c2ws'][:FEATURE_VIEWS],\n",
    "    'intrinsics': pose_source['intrinsics'][:FEATURE_VIEWS],\n",
    "    'near_fars': pose_source['near_fars'][:FEATURE_VIEWS]\n",
    "}\n",
    "\n",
    "# Color feature volume from multi-view images\n",
    "color_feat = build_color_volume(vox_pts, pose_source_feat, imgs_un_feat, with_mask=True)\n",
    "color_feat = color_feat.view(D, H, W, -1).unsqueeze(0).permute(0, 4, 1, 2, 3)  # [1, C, D, H, W]\n",
    "\n",
    "# Compute density over the voxel grid\n",
    "features = torch.cat((ref_volume.feat_volume, color_feat), dim=1).permute(0, 2, 3, 4, 1).reshape(D * H, W, -1)\n",
    "with torch.no_grad():\n",
    "    density = render_density(network_fn, vox_pts, features, network_query_fn)\n",
    "density_vol = density.reshape(D, H, W).detach().cpu()\n",
    "print('Density volume:', tuple(density_vol.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48caf895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point cloud size: 468583\n",
      "Saved PLY: results/mvsnerf_dtu/scan1_pointcloud.ply\n",
      "Saved HTML: results/mvsnerf_dtu/scan1_pointcloud.html\n"
     ]
    }
   ],
   "source": [
    "# Extract point cloud by thresholding density\n",
    "D, H, W = density_vol.shape\n",
    "coords = vox_pts.view(D, H, W, 3).cpu().numpy()\n",
    "sigma = density_vol.numpy()\n",
    "thr = np.percentile(sigma, 90)  # keep top 10% densest voxels\n",
    "mask = sigma > thr\n",
    "points = coords[mask]\n",
    "# Use reference view RGB for coloring\n",
    "rgb_vol = color_feat[0, 0:3].permute(1, 2, 3, 0).cpu().numpy()  # [D,H,W,3]\n",
    "colors = (np.clip(rgb_vol[mask], 0, 1) * 255).astype(np.uint8)\n",
    "print('Point cloud size:', points.shape[0])\n",
    "\n",
    "# Save PLY\n",
    "ply_path = OUT_DIR / f'{SCAN}_pointcloud.ply'\n",
    "with open(ply_path, 'w') as f:\n",
    "    f.write('ply\\nformat ascii 1.0\\n')\n",
    "    f.write(f'element vertex {points.shape[0]}\\n')\n",
    "    f.write('property float x\\nproperty float y\\nproperty float z\\n')\n",
    "    f.write('property uchar red\\nproperty uchar green\\nproperty uchar blue\\n')\n",
    "    f.write('end_header\\n')\n",
    "    for (x, y, z), (r, g, b) in zip(points, colors):\n",
    "        f.write(f'{x} {y} {z} {int(r)} {int(g)} {int(b)}\\n')\n",
    "print('Saved PLY:', ply_path)\n",
    "\n",
    "# Interactive point cloud HTML\n",
    "pc_fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=points[:,0], y=points[:,1], z=points[:,2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=1.5, color=['rgb(%d,%d,%d)' % tuple(c) for c in colors])\n",
    ")])\n",
    "pc_fig.update_layout(scene_aspectmode='data', title=f'{SCAN} Point Cloud')\n",
    "pc_html = OUT_DIR / f'{SCAN}_pointcloud.html'\n",
    "pc_fig.write_html(pc_html, include_plotlyjs='cdn')\n",
    "print('Saved HTML:', pc_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "019b71bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved OBJ: results/mvsnerf_dtu/scan1_mesh.obj\n",
      "Saved HTML: results/mvsnerf_dtu/scan1_mesh.html\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct mesh via marching cubes on density volume\n",
    "level = float(thr)\n",
    "verts, faces, normals, values = marching_cubes(volume=sigma, level=level, spacing=(1.0, 1.0, 1.0))\n",
    "verts_idx = np.clip(np.round(verts).astype(np.int64), [0,0,0], [D-1,H-1,W-1])\n",
    "world_coords = coords[verts_idx[:,0], verts_idx[:,1], verts_idx[:,2]]\n",
    "\n",
    "# Save a simple OBJ mesh\n",
    "obj_path = OUT_DIR / f'{SCAN}_mesh.obj'\n",
    "with open(obj_path, 'w') as f:\n",
    "    for v in world_coords:\n",
    "        f.write(f'v {v[0]} {v[1]} {v[2]}\\n')\n",
    "    for (a,b,c) in faces:\n",
    "        f.write(f'f {int(a)+1} {int(b)+1} {int(c)+1}\\n')\n",
    "print('Saved OBJ:', obj_path)\n",
    "\n",
    "# Interactive mesh HTML\n",
    "mesh_fig = go.Figure(data=[go.Mesh3d(\n",
    "    x=world_coords[:,0], y=world_coords[:,1], z=world_coords[:,2],\n",
    "    i=faces[:,0], j=faces[:,1], k=faces[:,2],\n",
    "    color='lightblue', opacity=0.5\n",
    ")])\n",
    "mesh_fig.update_layout(scene_aspectmode='data', title=f'{SCAN} Mesh')\n",
    "mesh_html = OUT_DIR / f'{SCAN}_mesh.html'\n",
    "mesh_fig.write_html(mesh_html, include_plotlyjs='cdn')\n",
    "print('Saved HTML:', mesh_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35a090cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HTML: results/mvsnerf_dtu/scan1_combined.html\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side visualization: input views + point cloud + mesh\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Build a simple montage from input views\n",
    "def make_montage(tensor_imgs, max_views=4):\n",
    "    # tensor_imgs: [1, V, 3, H, W] in 0..1\n",
    "    imgs_np = tensor_imgs[0].permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "    v = min(max_views, imgs_np.shape[0])\n",
    "    imgs_np = imgs_np[:v]\n",
    "    # pad to 4 for 2x2 grid\n",
    "    while imgs_np.shape[0] < 4:\n",
    "        imgs_np = np.concatenate([imgs_np, imgs_np[-1:]], axis=0)\n",
    "    top = np.concatenate([imgs_np[0], imgs_np[1]], axis=1)\n",
    "    bottom = np.concatenate([imgs_np[2], imgs_np[3]], axis=1)\n",
    "    montage = np.concatenate([top, bottom], axis=0)\n",
    "    return (np.clip(montage, 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "montage = make_montage(imgs_un_feat, max_views=FEATURE_VIEWS)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    specs=[[{\"type\": \"image\"}, {\"type\": \"scene\"}, {\"type\": \"scene\"}]],\n",
    "    column_widths=[0.35, 0.325, 0.325],\n",
    "    subplot_titles=[\"Input Views\", \"Point Cloud\", \"Mesh\"]\n",
    ")\n",
    "fig.add_trace(go.Image(z=montage), row=1, col=1)\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=points[:,0], y=points[:,1], z=points[:,2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=1.5, color=['rgb(%d,%d,%d)' % tuple(c) for c in colors])\n",
    "), row=1, col=2)\n",
    "fig.add_trace(go.Mesh3d(\n",
    "    x=world_coords[:,0], y=world_coords[:,1], z=world_coords[:,2],\n",
    "    i=faces[:,0], j=faces[:,1], k=faces[:,2],\n",
    "    color='lightblue', opacity=0.5\n",
    "), row=1, col=3)\n",
    "fig.update_layout(height=500, showlegend=False, scene_aspectmode='data', scene2_aspectmode='data')\n",
    "\n",
    "# Save combined visualization to HTML (browser-friendly without nbformat)\n",
    "combined_html = OUT_DIR / f'{SCAN}_combined.html'\n",
    "fig.write_html(combined_html, include_plotlyjs='cdn')\n",
    "print('Saved HTML:', combined_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b870c33",
   "metadata": {},
   "source": [
    "## Usage\n",
    "- Set `DTU_ROOT`, `SCAN`, and `MVSNERF_CKPT` environment variables or edit the config cell.\n",
    "- Run cells 1â†’7. Outputs are saved under `results/mvsnerf_dtu/`.\n",
    "- Open the HTML files in your browser: point cloud and mesh visualizations.\n",
    "\n",
    "Example to run in terminal before opening the notebook:\n",
    "\n",
    "```bash\n",
    "export DTU_ROOT=/data/DTU\n",
    "export DTU_SCAN=scan1\n",
    "export MVSNERF_CKPT=runs_fine_tuning/scan1-ft/ckpts/latest.tar\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mvsnerf]",
   "language": "python",
   "name": "conda-env-mvsnerf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
